{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0a71b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "019706c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = 'REDACTED'\n",
    "secret_key = 'REDACTED'\n",
    "os.environ['TOKEN'] = 'REDACTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7997620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7930b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0aa3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e53ac20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48000ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"airlines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "185dffa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAMJPsQEAAAAAayfUMLVJZtEVhEl837E2ZjDHbb8%3D2AQ4LGDS5kFv68o0rXA1PqkMsk9DSTuy2fKfXa4Go6HWPXVtYO'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02d4e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = create_url(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2369b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = connect_to_endpoint(url[0], headers, url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Sentiment_Analysis_of_{}_Tweets_About_{}.csv'.format(number, query)\n",
    "\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.DictWriter(\n",
    "       f=csvfile,\n",
    "       fieldnames=[\"Tweet\", \"Sentiment\"]\n",
    "    )\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    print(\"--- Opened a CSV file to store the results of your sentiment analysis... \\n\")\n",
    "\n",
    "## tidy up the Tweets and send each to the AYLIEN Text API\n",
    "    for c, result in enumerate(results, start=1):\n",
    "        tweet = result.text\n",
    "        tidy_tweet = tweet.strip().encode('ascii', 'ignore')\n",
    "\n",
    "        if len(tweet) == 0:\n",
    "            print('Empty Tweet')\n",
    "            continue\n",
    "\n",
    "        response = client.Sentiment({'text': tidy_tweet})\n",
    "        csv_writer.writerow({\n",
    "           'Tweet': response['text'],\n",
    "           'Sentiment': response['polarity']\n",
    "        })\n",
    "\n",
    "        print(\"Analyzed Tweet {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "#import for bag of word\n",
    "import numpy as np\n",
    "#For the regular expression\n",
    "import re\n",
    "#Textblob dependency\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "#set to string \n",
    "from ast import literal_eval\n",
    "#From src dependency \n",
    "from sentencecounter import no_sentences,getline,gettempwords \n",
    "\n",
    "import os\n",
    "def getsysets(word):\n",
    "    syns = wordnet.synsets(word)  #wordnet from ntlk.corpus  will not work with textblob\n",
    "    #print(syns[0].name()) \n",
    "    #print(syns[0].lemmas()[0].name())  #get synsets names \n",
    "    #print(syns[0].definition())  #defination \n",
    "    #print(syns[0].examples())    #example\n",
    "\n",
    "\n",
    "# getsysets(\"good\")\n",
    "\n",
    "\n",
    "def getsynonyms(word):\n",
    "    synonyms = []\n",
    "    # antonyms = []\n",
    " \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            # if l.antonyms():\n",
    "            #     antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "    # print(set(synonyms))\n",
    "    return(set(synonyms))\n",
    "    # print(set(antonyms))\n",
    "\n",
    "\n",
    "# getsynonyms_and_antonyms(\"good\")\n",
    "\n",
    "\n",
    "def extract_words(sentence):\n",
    "    ignore_words = ['a']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned    \n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = extract_words(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence)\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)\n",
    "\n",
    "def tokenizer(sentences):\n",
    "    token = word_tokenize(sentences)\n",
    "    return token\n",
    "    print(\"#\"*100)\n",
    "    print (sent_tokenize(sentences))\n",
    "    print (token)\n",
    "    print(\"#\"*100)\n",
    "\n",
    "\n",
    "# sentences = \"Machine learning is great\",\"Natural Language Processing is a complex field\",\"Natural Language Processing is used in machine learning\"\n",
    "# vocabulary = tokenize_sentences(sentences)\n",
    "# print (vocabulary)\n",
    "# tokenizer(sentences)\n",
    "\n",
    "def createposfile(filename,word):\n",
    "    # filename = input(\"Enter destination file name in string format :\")\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(word+'\\n')\n",
    "\n",
    "def createnegfile(filename,word):\n",
    "    # filename = input(\"Enter destination file name in string format :\")\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(word)\n",
    "\n",
    "def getsortedsynonyms(word):\n",
    "    sortedsynonyms = sorted(getsynonyms(word))\n",
    "    return sortedsynonyms\n",
    "\n",
    "def getlengthofarray(word):\n",
    "    return getsortedsynonyms(word).__len__()\n",
    "\n",
    "def readposfile():\n",
    "    f = open('list of positive words.txt')\n",
    "    return f\n",
    "\n",
    "# def searchword(word, sourcename):\n",
    "#     if word in open('list of negative words.txt').read():\n",
    "#             createnegfile('destinationposfile.txt',word)\n",
    "#     elif word in open('list of positive words.txt').read():\n",
    "#             createposfile('destinationnegfile.txt',word)     \n",
    "\n",
    "#     else:\n",
    "#         for i in range (0,getlengthofarray(word)):\n",
    "#             searchword(getsortedsynonyms(word)[i],sourcename)\n",
    "\n",
    "def searchword(word,srcfile):\n",
    "    # if word in open('list of negative words.txt').read():\n",
    "    #         createnegfile('destinationposfile.txt',word)\n",
    "    if word in open('list of positive words.txt').read():\n",
    "            createposfile('destinationnegfile.txt',word)\n",
    "    else:\n",
    "        for i in range(0,getlengthofarray(word)):\n",
    "            searchword(sorted(getsynonyms(word))[i],srcfile)\n",
    "            f = open(srcfile,'w')\n",
    "            f.writelines(word)\n",
    "\n",
    "print ('#'*50)\n",
    "# searchword('lol','a.txt')\n",
    "print(readposfile())\n",
    "# tokenizer(sentences)\n",
    "# getsynonyms('good')\n",
    "# print(sorted(getsynonyms('good'))[2])  #finding an array object [hear it's 3rd object]\n",
    "print ('#'*50)\n",
    "# print (getsortedsynonyms('bad').__len__())\n",
    "# createposfile('created.txt','lol')\n",
    "# for word in word_tokenize(getline()):\n",
    "#     searchword(word,'a.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0ba87496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text\": \"@VirginAmerica What @dhepburn said. ...\",\"truncated\": true,\"in_reply_to_user_id\": null, \"in_reply_to_status_id\": null, \"favorited\": false, \"source\": \"<a href=\"http://twitter.com/\" rel=\"nofollow\">Twitter for iPhone</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_status_id_str\": null, \"id_str\": \"54691802283900928\", \"entities\": {\"user_mentions\": [{\"indices\": [3, 19], \"screen_name\": \"PostGradProblem\", \"id_str\": \"271572434\", \"name\": \"PostGradProblems\", \"id\": 271572434}], \"urls\": [ ], \"hashtags\": [ ]}, \"contributors\": null, \"retweeted\": false, \"in_reply_to_user_id_str\": null, \"place\": null, \"retweet_count\": 4, \"created_at\": \"Sun Apr 03 23:48:36 +0000 2011\", \"retweeted_status\": {\"text\": \"In preparation for the NFL lockout, I will be spending twice as much time analyzing my fantasy baseball team during company time. #PGP\", \"truncated\": false, \"in_reply_to_user_id\": null, \"in_reply_to_status_id\": null, \"favorited\": false, \"source\": \"<a href=\"http://www.hootsuite.com\" rel=\"nofollow\">HootSuite</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_status_id_str\": null, \"id_str\": \"54640519019642881\", \"entities\": {\"user_mentions\": [ ], \"urls\": [ ], \"hashtags\": [{\"text\": \"PGP\", \"indices\": [130, 134]}]}, \"contributors\": null, \"retweeted\": false, \"in_reply_to_user_id_str\": null, \"place\": null, \"retweet_count\": 4, \"created_at\": \"Sun Apr 03 20:24:49 +0000 2011\", \"user\": {\"notifications\": null, \"profile_use_background_image\": true, \"statuses_count\": 31, \"profile_background_color\": \"C0DEED\", \"followers_count\": 3066, \"profile_image_url\": \"http://a2.twimg.com/profile_images/1285770264/PGP_normal.jpg\", \"listed_count\": 6, \"profile_background_image_url\": \"http://a3.twimg.com/a/1301071706/images/themes/theme1/bg.png\", \"description\": \"\",\"screen_name\": \"PostGradProblem\", \"default_profile\": true, \"verified\": false, \"time_zone\": null, \"profile_text_color\": \"333333\", \"is_translator\": false, \"profile_sidebar_fill_color\": \"DDEEF6\", \"location\": \"\", \"id_str\": \"271572434\", \"default_profile_image\": false, \"profile_background_tile\": false, \"lang\": \"en\", \"friends_count\": 21, \"protected\": false, \"favourites_count\": 0, \"created_at\": \"Thu Mar 24 19:45:44 +0000 2011\", \"profile_link_color\": \"0084B4\", \"name\": \"PostGradProblems\", \"show_all_inline_media\": false, \"follow_request_sent\": null, \"geo_enabled\": false, \"profile_sidebar_border_color\": \"C0DEED\", \"url\": null, \"id\": 271572434, \"contributors_enabled\": false, \"following\": null, \"utc_offset\": null}, \"id\": 54640519019642880, \"coordinates\": null, \"geo\": null}, \"user\": {\"notifications\": null, \"profile_use_background_image\": true, \"statuses_count\": 351, \"profile_background_color\": \"C0DEED\", \"followers_count\": 48, \"profile_image_url\": \"http://a1.twimg.com/profile_images/455128973/gCsVUnofNqqyd6tdOGevROvko1_500_normal.jpg\", \"listed_count\": 0, \"profile_background_image_url\": \"http://a3.twimg.com/a/1300479984/images/themes/theme1/bg.png\", \"description\": \"watcha doin in my waters?\", \"screen_name\": \"OldGREG85\", \"default_profile\": true, \"verified\": false, \"time_zone\": \"Hawaii\", \"profile_text_color\": \"333333\", \"is_translator\": false, \"profile_sidebar_fill_color\": \"DDEEF6\", \"location\": \"Texas\", \"id_str\": \"80177619\", \"default_profile_image\": false, \"profile_background_tile\": false, \"lang\": \"en\", \"friends_count\": 81,\"protected\": false, \"favourites_count\": 0, \"created_at\": \"Tue Oct 06 01:13:17 +0000 2009\", \"profile_link_color\": \"0084B4\", \"name\": \"GG\", \"show_all_inline_media\": false, \"follow_request_sent\": null, \"geo_enabled\": false, \"profile_sidebar_border_color\": \"C0DEED\", \"url\": null, \"id\": 80177619, \"contributors_enabled\": false, \"following\": null, \"utc_offset\": -36000}, \"id\": 54691802283900930, \"coordinates\": null, \"geo\": null}'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440be21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
